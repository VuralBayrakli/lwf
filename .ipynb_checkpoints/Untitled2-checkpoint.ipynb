{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01744251",
   "metadata": {},
   "source": [
    "### Kütüphanelerin import edilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee47fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import scipy.io as scio\n",
    "from scipy.io import loadmat\n",
    "import torchvision\n",
    "import os\n",
    "import argparse\n",
    "import progress_bar\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import pdb\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tfs\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86dfdcc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x1f7a362f520>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c913545",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = [\"flowers_train2\", \"flowers_test2\"]\n",
    "data_transforms = {\n",
    "    liste[0]: transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    liste[1]: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = \"C:\\\\users\\\\vuralbayrakli\\\\lwf\"  # veri kümesinin ana dizini\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [liste[0], liste[1]]}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in [liste[0], liste[1]]}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [liste[0], liste[1]]}\n",
    "class_names = image_datasets[liste[0]].classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d5b912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5683eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['TRAIN', 'TEST']:\n",
    "                if phase == 'TRAIN':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'TRAIN'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'TRAIN':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'TRAIN':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'TEST' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c15de0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Here the size of each output sample is set to 2.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model_ft\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_ftrs, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m model_ft \u001b[38;5;241m=\u001b[39m model_ft\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[0;32m     10\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Observe that all parameters are being optimized\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff8ba552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.5764e-04, -4.2829e-02, -1.6585e-02,  ..., -4.2671e-02,\n",
       "         -4.3124e-02,  3.0809e-02],\n",
       "        [ 4.0476e-02, -6.3135e-03, -2.6779e-02,  ...,  3.4232e-02,\n",
       "         -1.3618e-03,  4.2376e-02],\n",
       "        [-3.3842e-02,  1.4523e-02, -3.4298e-02,  ...,  3.8237e-02,\n",
       "         -3.0083e-02, -2.1653e-02],\n",
       "        ...,\n",
       "        [ 3.4443e-02,  1.8961e-02,  3.1957e-02,  ...,  3.7320e-02,\n",
       "         -2.7385e-03, -2.9132e-02],\n",
       "        [ 2.4400e-02,  2.0640e-03,  2.1835e-02,  ..., -4.0427e-02,\n",
       "         -2.6602e-02, -3.0665e-02],\n",
       "        [ 8.0334e-03,  1.9673e-02,  2.8463e-02,  ...,  3.5149e-02,\n",
       "         -3.9404e-02,  8.4192e-06]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f06820",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_new_class = 5\n",
    "model_ = models.resnet18()\n",
    "model2_ = models.resnet18()\n",
    "\n",
    "num_ftrs = model_.fc.in_features\n",
    "model_.fc = nn.Linear(num_ftrs, 5)\n",
    "model2_.fc = nn.Linear(num_ftrs, 5)\n",
    "model_.load_state_dict(torch.load('best_model_params.pt'))\n",
    "model2_.load_state_dict(torch.load('best_model_params.pt'))\n",
    "out_features = model_.fc.out_features\n",
    "\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dd4a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = model_.fc.weight.data\n",
    "bias = model_.fc.bias.data\n",
    "new_out_features = num_new_class+out_features\n",
    "new_fc = nn.Linear(num_ftrs, new_out_features)\n",
    "new_fc.weight.data[:out_features] = weight\n",
    "new_fc.bias.data[:out_features] = bias\n",
    "# Replace the old FC layer\n",
    "model_.fc = new_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30f6d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T=2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Used to balance the new class loss1 and the old class loss2\n",
    "# Loss1 is the cross entropy between output of the new task and label\n",
    "# Loss2 is the cross entropy between output of the old task and output of the old model\n",
    "# It should be noticed that before calculating loss2, the output of each model should- \n",
    "# -be handled by the new softmax \n",
    "alpha = 0.01\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model_.parameters()), lr=0.01,\n",
    "        momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "model_params_path = os.path.join(data_dir, 'model_params3.pt')\n",
    "            \n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model2_.eval()\n",
    "    model_.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    since = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloaders[liste[0]]):\n",
    "        \n",
    "        inputs, targets = inputs.to(\"cpu\"), targets.to(\"cpu\")\n",
    "        targets += out_features\n",
    "        targets = targets.to(torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_(inputs)\n",
    "        outputs_ = torch.argmax(outputs, dim=1)\n",
    "        outputs_ = outputs_.to(torch.float)\n",
    "        soft_target = model2_(inputs)\n",
    "        # Cross entropy between output of the new task and label\n",
    "        loss1 = criterion(outputs_,targets)\n",
    "        # Using the new softmax to handle outputs\n",
    "        outputs_S = F.softmax(outputs[:,:out_features]/T,dim=1)\n",
    "        outputs_T = F.softmax(soft_target[:,:out_features]/T,dim=1)\n",
    "        # Cross entropy between output of the old task and output of the old model\n",
    "        loss2 = outputs_T.mul(-1*torch.log(outputs_S))\n",
    "        loss2 = loss2.sum(1)\n",
    "        loss2 = loss2.mean()*T*T\n",
    "        loss = loss1*alpha+loss2*(1-alpha)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)' % (train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "    time_elapsed = time.time() - since\n",
    "    torch.save(model_.state_dict(), model_params_path)\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    return train_loss/(batch_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1ed64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c516b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# model_ ve dataloaders[\"TEST\"] örnek olarak verilmiş olsun\n",
    "# model_ nesnesini değerlendirme moduna alın\n",
    "model_.eval()\n",
    "\n",
    "predictions = []  # Tahminleri saklamak için boş bir liste oluşturun\n",
    "true_labels = []  # Gerçek etiketleri saklamak için boş bir liste oluşturun\n",
    "\n",
    "# Modeli değerlendirme modunda kullanarak tahminler yapın\n",
    "with torch.no_grad():  # Gradient hesaplama yapmamak için torch.no_grad() kullanın\n",
    "    for inputs, labels in dataloaders[liste[1]]:\n",
    "        inputs = inputs.to(\"cpu\")  # Girdi verilerini uygun cihaza taşıyın (örneğin, GPU'ya)\n",
    "        labels = labels.to(\"cpu\")  # Etiketleri uygun cihaza taşıyın\n",
    "\n",
    "        # Modelden geçirme\n",
    "        outputs = model_(inputs)\n",
    "\n",
    "        # Tahminleri ve gerçek etiketleri listelere ekleyin\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())  # Tahminleri CPU'ya taşıyarak ekleyin\n",
    "        true_labels.extend(labels.cpu().numpy())  # Gerçek etiketleri CPU'ya taşıyarak ekleyin\n",
    "\n",
    "# Tahminleri ve gerçek etiketleri kullanarak değerlendirme veya başka işlemler yapabilirsiniz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16d9b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "for predicted, true_label in zip(predictions, true_labels):\n",
    "    if predicted == true_label:\n",
    "        correct_predictions += 1\n",
    "    total_samples += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c9c79a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = correct_predictions / total_samples\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0ce9b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = correct_predictions / total_samples\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d03fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f9a8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaafd22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff8244c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Görüntüyü yükleyin (örneğin, \"image.jpg\" dosyasını yükleyin)\n",
    "image_path = \"C:/Users/VuralBayrakli/datasets/flower_photos/tulips/12517756805_56b74be742.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Görüntüyü PyTorch tensörüne dönüştürmek için transform kullanın\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Görüntüyü belirli bir boyuta yeniden boyutlandırın\n",
    "    transforms.ToTensor(),  # Görüntüyü tensöre dönüştürün\n",
    "])\n",
    "\n",
    "# Transformu kullanarak görüntüyü PyTorch tensörüne dönüştürün\n",
    "input_tensor = transform(image)\n",
    "input_tensor = input_tensor.unsqueeze(0)  # Batch boyutunu ekleyin (batch boyutu 1)\n",
    "\n",
    "# Giriş tensörü şimdi modelinize geçirilebilir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "332ac5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_(input_tensor)\n",
    "\n",
    "# Softmax fonksiyonunu doğrudan uygulayın\n",
    "softmax_output = torch.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b8d8a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0686e-04, 9.2100e-04, 4.9249e-03, 2.6524e-06, 9.9404e-01]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d4383de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(dataloaders[liste[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b553c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7010f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "T=2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "inputs, targets = img.to(\"cpu\"), labels.to(\"cpu\")\n",
    "\n",
    "targets = targets.to(torch.float)\n",
    "\n",
    "outputs = model_(inputs)\n",
    "outputs_ = torch.argmax(outputs, dim=1)\n",
    "outputs_ = outputs_.to(torch.float)\n",
    "soft_target = model2_(inputs)\n",
    "# Cross entropy between output of the new task and label\n",
    "loss1 = criterion(outputs_,targets)\n",
    "# Using the new softmax to handle outputs\n",
    "outputs_S = F.softmax(outputs[:,:out_features]/T,dim=1)\n",
    "outputs_T = F.softmax(soft_target[:,:out_features]/T,dim=1)\n",
    "# Cross entropy between output of the old task and output of the old model\n",
    "loss2 = outputs_T.mul(-1*torch.log(outputs_S))\n",
    "loss2 = loss2.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f7316cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = loss2.mean()*T*T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "52b9ba35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0884, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7bd95601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1679, 0.2225, 0.4387, 0.0709, 0.1001],\n",
       "        [0.0159, 0.0102, 0.1208, 0.0160, 0.8371],\n",
       "        [0.1335, 0.5389, 0.1765, 0.0506, 0.1005],\n",
       "        [0.1251, 0.0180, 0.0748, 0.7647, 0.0174]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0345bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
